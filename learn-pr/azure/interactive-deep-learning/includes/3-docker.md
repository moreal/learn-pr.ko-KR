## <a name="docker"></a><span data-ttu-id="bf477-101">Docker</span><span class="sxs-lookup"><span data-stu-id="bf477-101">Docker</span></span>

![Docker 로고](../media/3-image1.PNG)

<span data-ttu-id="bf477-103">개발자는 Docker를 활용하여 응용 프로그램을 사실상 어디서나 실행 가능하며 필요한 기능을 모두 포함하는 경량의 휴대용 컨테이너로 손쉽게 압축, 제공 및 실행할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-103">Docker enables developers to easily pack, ship, and run any  application as a lightweight, portable, self-sufficient container, which can run virtually anywhere.</span></span> <span data-ttu-id="bf477-104">DSVM 기본 이미지에 가장 널리 사용되는 심층 학습 프레임워크가 미리 설치되어 있는데 Docker와 같은 컨테이너화 클라이언트가 필요한 이유가 궁금하실 것입니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-104">If the DSVM base image comes with the most popular deep learning frameworks pre-installed, why is there a need for containerization clients such as Docker?</span></span>

<span data-ttu-id="bf477-105">개발자가 심층 학습 작업을 실행하려 할 때는 다음과 같은 종속성 문제가 흔히 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-105">Often when attempting to run deep learning tasks developers find themselves facing dependency nightmares, such as:</span></span> 

- <span data-ttu-id="bf477-106">사용자 지정 패키지를 빌드해야 함 - 심층 학습 연구원들은 GitHub에 코드를 게시할 때 프로덕션 환경에 대해서는 크게 고려하지 않는 경향이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-106">Having to build custom packages - Deep learning researchers tend to think less about production when they publish code to Git Hub.</span></span> <span data-ttu-id="bf477-107">즉, 연구원의 개발 환경에서 패키지가 작동하면 다른 사용자도 패키지를 실행할 수 있다고 가정하는 경우가 많습니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-107">If they can get a package working on their own development environment, they often just assume that others will be able to do so as well.</span></span>
- <span data-ttu-id="bf477-108">GPU 드라이버 버전 관리 - CUDA는 Nvidia에서 개발한 병렬 컴퓨팅 플랫폼이자 API(응용 프로그래밍 인터페이스)입니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-108">GPU driver versioning - CUDA is a parallel computing platform and application programming interface (API) developed by Nvidia.</span></span> <span data-ttu-id="bf477-109">CUDA를 사용하는 소프트웨어 개발자 및 소프트웨어 엔지니어는 CUDA 사용 가능 GPU(그래픽 처리 장치)를 범용 처리에 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-109">It allows software developers and software engineers to use a CUDA-enabled graphics processing unit (GPU) for general-purpose processing.</span></span> <span data-ttu-id="bf477-110">특정 Tensorflow 버전은 9.1 이후 버전의 CUDA와 연동되지 않습니다. 하지만 PyTorch 등의 기타 프레임워크는 최신 버전의 CUDA에서 더 효율적으로 작동하는 것으로 확인되었습니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-110">Certain versions of Tensorflow will not work with versions of CUDA above 9.1 but other frameworks such as PyTorch seem to perform better with later versions of CUDA.</span></span>

<span data-ttu-id="bf477-111">이러한 문제를 해결하고 코드의 유용성을 개선하려는 경우 Docker 또는 Docker의 GPU 변형인 Nvidia-Docker를 사용하여 심층 학습 프로젝트를 관리하고 실행할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="bf477-111">To get around these issues and to increase the usability of code, you can use Docker or its GPU variant Nvidia-Docker to manage and run deep learning projects.</span></span> 

<!--Quiz 
What is CUDA? 
What versioning issues do deep learning engineers deal with? -->